{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp97VfrVPVpv"
      },
      "source": [
        "# 1. Librerías & Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCinFr1LPVO5",
        "outputId": "597670c9-b296-40b2-bafe-8715c6143599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=494cfa5376d796cf61726721db28fba3a89b959bead45f05e122d84350020f71\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zH4iyjyxPeYT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Uy3cfH_iPh4j"
      },
      "outputs": [],
      "source": [
        "#import neo4j [TO-EDIT]\n",
        "#from neo4j import GraphDatabase\n",
        "#import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "61FWrL1FQC9j",
        "outputId": "b2d7283d-3491-4b46-ad37-05d332df1255"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3b1035087967:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc # Elemento que ejecuta toda instrucción."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FFiAi4oQNPj"
      },
      "source": [
        "# 2. Neo4j Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GGkPemU0Qp5t"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6vbD7A2lQ0Sw"
      },
      "outputs": [],
      "source": [
        "graph = [(1,11,2),(1,11,3),(2,11,3),(3,11,2),(3,11,4),(4,11,1),(4,11,2),(4,11,3),(4,12,5),(5,12,1),(5,12,2),(5,12,6)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcJZG8CQQQne"
      },
      "source": [
        "# 3. MapReduce Algorithm for Triangles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbPXryMmSVQM",
        "outputId": "281afe7a-741c-43d5-f05f-ecb7f6adbe98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 11, 2),\n",
              " (1, 11, 3),\n",
              " (2, 11, 3),\n",
              " (3, 11, 2),\n",
              " (3, 11, 4),\n",
              " (4, 11, 1),\n",
              " (4, 11, 2),\n",
              " (4, 11, 3),\n",
              " (4, 12, 5),\n",
              " (5, 12, 1),\n",
              " (5, 12, 2),\n",
              " (5, 12, 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "rdd_graph = sc.parallelize(graph)\n",
        "rdd_graph.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t9dJKE1fVsAw"
      },
      "outputs": [],
      "source": [
        "def hash(number):\n",
        "  \"\"\"\n",
        "  Returns number mod 2. The ouput will be 0 or 1.\n",
        "  \"\"\"\n",
        "  return number % 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SQgndLX_S5RZ"
      },
      "outputs": [],
      "source": [
        "def get_keys(edge, b_dim, b_set, pattern_dim):\n",
        "  \"\"\"\n",
        "  output: retorna las llaves correspondientes para un vertice.\n",
        "\n",
        "  Idea general: buscamos el par hash_n1,hash_n2 dentro de las posibles\n",
        "  combinaciones dentro del espacio de imagenes de la funcion de hash.\n",
        "  Dentro del for, obtenemos un string con la codificacion de las llaves y luego\n",
        "  verificamos si es una llave candidata para el vertice entregado:\n",
        "\n",
        "  1.  sequence_in_reducer: Si la secuencia 'b1b2' esta en la llave del reducer\n",
        "      codificada como hash(n1)hash(n2) ?, donde ? = 0 o 1, entonces se considerará\n",
        "      el par reducer_key : edge.\n",
        "  2.  edge_case: El otro caso, es para cuando tenemos por ejemplo x = n2 y z = n2\n",
        "      para patrones de 3 vertices.\n",
        "  \"\"\"\n",
        "\n",
        "  hash_n1 = hash(edge[0]) # valor de hash para el nodo 1 = b1\n",
        "  hash_n2 = hash(edge[2]) # valor de hash para el nodo 2 = b2\n",
        "  values = [] # posible keys\n",
        "  sequence = '{}{}'.format(hash_n1, hash_n2)\n",
        "\n",
        "  for i in range(0, b_dim ** pattern_dim):\n",
        "    reducer = ''.join(str(num) for num in b_set[i])\n",
        "    sequence_in_reducer = sequence in reducer\n",
        "    edge_case = reducer[0] == sequence[1] and reducer[pattern_dim - 1] == sequence[0]\n",
        "    if sequence_in_reducer or edge_case:\n",
        "        reducer_key = tuple(int(digit) for digit in reducer)\n",
        "        values.append((reducer_key, edge))\n",
        "\n",
        "  return values\n",
        "\n",
        "\n",
        "def map_phase(rdd, b_dim, b_set, pattern_dim):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    - rdd: RDD del grafo de dimension 'dim'\n",
        "    - b_dim: Cantidad de elementos de las imagenes de la funcion de hash.\n",
        "    - b_set: Imagenes de la funcion de hash.\n",
        "    - pattern_dim: cantidad de nodos del patron de grafo.\n",
        "  ouput: Mapeo de cada arista con respecto a las llaves\n",
        "  \"\"\"\n",
        "\n",
        "  mapped_keys = rdd.flatMap(lambda edge: get_keys(edge, b_dim, b_set, pattern_dim))\n",
        "  reducers = mapped_keys.groupByKey().mapValues(list)\n",
        "  return reducers\n",
        "\n",
        "def reduce_phase(reducers, pattern_dim):\n",
        "  \"\"\"\n",
        "  input: RDD del grafo y cantidad de nodos del patron de grafo.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "B = 2 # Dimension de elementos del conjunto de imagenes de la funcion de hash: |{0,1}|\n",
        "L = 3 # Dimension del patron de grafo (triangulo para este caso)\n",
        "BSET = list(range(B))\n",
        "REDUCERS = list(product(BSET, repeat=L))\n",
        "\n",
        "# Fase de Map: Obtenemos las llaves de cada reducer y el conjunto de aristas mapeados a estas llaves.\n",
        "#\n",
        "reducers = map_phase(rdd_graph, B, REDUCERS, L)\n",
        "# Fase Reduce: Obtenemos todos los posibles patrones de L nodos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reducers.collect()"
      ],
      "metadata": {
        "id": "3rdOfGfOEEug",
        "outputId": "083b7a3b-fadf-4b44-8131-3d3fe7409e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, 0, 1),\n",
              "  [(1, 11, 2),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 2),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((0, 1, 0),\n",
              "  [(1, 11, 2),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 2),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((1, 0, 0),\n",
              "  [(1, 11, 2),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 2),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((1, 1, 1), [(1, 11, 3), (5, 12, 1)]),\n",
              " ((0, 1, 1),\n",
              "  [(1, 11, 2),\n",
              "   (1, 11, 3),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 1),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((1, 0, 1),\n",
              "  [(1, 11, 2),\n",
              "   (1, 11, 3),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 1),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((1, 1, 0),\n",
              "  [(1, 11, 2),\n",
              "   (1, 11, 3),\n",
              "   (2, 11, 3),\n",
              "   (3, 11, 2),\n",
              "   (3, 11, 4),\n",
              "   (4, 11, 1),\n",
              "   (4, 11, 3),\n",
              "   (4, 12, 5),\n",
              "   (5, 12, 1),\n",
              "   (5, 12, 2),\n",
              "   (5, 12, 6)]),\n",
              " ((0, 0, 0), [(4, 11, 2)])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_patterns(edges, pattern_dim):\n",
        "  neighbors = []\n",
        "  for i in range(len(edges)):\n",
        "    curr_edge = edges[i]\n",
        "    neighbors.append([])\n",
        "    for j in range(len(edges)):\n",
        "      next_edge = edges[j]\n",
        "      if next_edge == curr_edge:\n",
        "        continue\n",
        "      if curr_edge[2] == next_edge[0]:\n",
        "        neighbors[i].append(next_edge)\n",
        "  cycles = find_cycles(edges, neighbors, pattern_dim)\n",
        "\n",
        "  return cycles\n",
        "\n",
        "\n",
        "def find_cycles(edges, neighbors, pattern_dim):\n",
        "    cycles = []\n",
        "    for edge in edges: # Por cada vértice, verificamos si existe un ciclo.\n",
        "        visited = [False for i in range(len(edges))] # Restauramos la lista de nodos visitados\n",
        "        init_edge = edge\n",
        "        stack = [(init_edge, [init_edge])]\n",
        "        while len(stack):\n",
        "            # Extraemos una arista del stack\n",
        "            curr_edge, path = stack.pop()\n",
        "            id_curr_edge = edges.index(curr_edge)\n",
        "            # Marcamos la arista como visitada\n",
        "            visited[id_curr_edge] = True\n",
        "\n",
        "            # En el caso que esta arista tenga un camino de largo pattern_dim,\n",
        "            # entonces el posible que presente un ciclo de 'pattern_dim' aristas.\n",
        "            if len(path) == pattern_dim:\n",
        "                last_node = path[pattern_dim - 1][2] # primer nodo del camino\n",
        "                first_node = path[0][0] # ultimo nodo del camino\n",
        "                if last_node == first_node: # si es el mismo nodo, entonces hay ciclo\n",
        "                  # n1, n2, n3\n",
        "                  nodes = tuple(elem[0] for elem in path)\n",
        "                  cycles.append(nodes)\n",
        "                continue # seguimos iterando\n",
        "\n",
        "            for neighbor_edge in neighbors[edges.index(curr_edge)]:\n",
        "                if not visited[edges.index(neighbor_edge)]:\n",
        "                    stack.append((neighbor_edge, path + [neighbor_edge]))\n",
        "\n",
        "    return cycles\n",
        "\n",
        "\n",
        "pattern_dim = 4\n",
        "\n",
        "reducers_edges = reducers.map(lambda v: v[1]).filter(lambda x: len(x) >= pattern_dim)\n",
        "\n",
        "patterns = reducers_edges.map(lambda edges: find_patterns(edges, pattern_dim))\n",
        "all_patterns = patterns\n",
        "all_patterns.flatMap(list).distinct().collect()\n",
        "\n",
        "#reducers.map(lambda message: find_patterns(message, pattern_dim)).collect()"
      ],
      "metadata": {
        "id": "rKdS2opbCV9K",
        "outputId": "d3655e15-266c-4ae7-a9e8-cd55b2458ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3, 4),\n",
              " (2, 3, 4, 5),\n",
              " (2, 3, 4, 3),\n",
              " (2, 3, 4, 1),\n",
              " (3, 2, 3, 4),\n",
              " (3, 4, 5, 2),\n",
              " (4, 1, 2, 3),\n",
              " (4, 5, 2, 3),\n",
              " (5, 2, 3, 4),\n",
              " (1, 3, 4, 5),\n",
              " (3, 4, 5, 1),\n",
              " (5, 1, 3, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4cg3Y5nbouR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs(edges):\n",
        "  patterns = []\n",
        "  visited = [False for i in range(len(edges))]\n",
        "  stack = []\n",
        "\n",
        "  init_edge = edges[0] # inicializamos el stack con la primera arista\n",
        "  stack.append((init_edge, [init_edge])) # arista y camino recorrido hasta el nodo actual\n",
        "\n",
        "  while len(stack) > 0:\n",
        "    curr_edge, edge_path = stack.pop() # Extraemos una arista del stack\n",
        "    curr_edge_idx = edges.index(curr_edge)\n",
        "\n",
        "    if (not visited[curr_edge_idx]):\n",
        "      visited[curr_edge_idx] = True\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdIXRQUiNB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aES7v_zWQZZL"
      },
      "source": [
        "# 4. MapReduce Algorithm for Squares\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRsbqVb-Qidy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}